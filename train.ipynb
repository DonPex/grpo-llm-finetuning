{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac624a22",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e559f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install sqlglot sqlparse\n",
    "%pip install torch\n",
    "%pip install tqdm pandas unsloth trl\n",
    "%pip install vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9214923d",
   "metadata": {},
   "source": [
    "# Reward generation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0817bba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import tempfile\n",
    "import sqlite3\n",
    "import logging\n",
    "import sqlparse\n",
    "import torch\n",
    "import copy\n",
    "from sqlglot import parse, transpile, ParseError\n",
    "from sqlglot.expressions import Column, Table\n",
    "from typing import List, Dict, Tuple, Any, Optional, Set, Union\n",
    "\n",
    "log_level = logging.DEBUG if os.environ.get(\n",
    "    \"SQL_DEBUG_MODE\") == \"1\" else logging.CRITICAL + 1\n",
    "logging.basicConfig(\n",
    "    level=log_level,\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "REWARD_WEIGHTS = {\n",
    "    \"format\": 1.0,\n",
    "    \"sql_correctness\": 1.2,\n",
    "    \"complexity\": 0.6,\n",
    "    \"reasoning\": 0.7,\n",
    "}\n",
    "\n",
    "DEBUG_MODE = os.environ.get(\"SQL_DEBUG_MODE\") == \"1\"\n",
    "\n",
    "ERR_SYNTAX = \"syntax_error\"\n",
    "ERR_MISSING_TABLE = \"missing_table\"\n",
    "ERR_MISSING_COLUMN = \"missing_column\"\n",
    "ERR_AMBIGUOUS_COLUMN = \"ambiguous_column\"\n",
    "ERR_TYPE_MISMATCH = \"type_mismatch\"\n",
    "ERR_CONSTRAINT = \"constraint_violation\"\n",
    "ERR_FUNCTION = \"function_error\"\n",
    "ERR_RESOURCE = \"resource_error\"\n",
    "ERR_OTHER = \"other_error\"\n",
    "ERR_SCHEMA_SETUP = \"schema_setup_error\"\n",
    "ERR_CONVERSION = \"sql_conversion_error\"\n",
    "ERR_EXECUTION = \"sql_execution_error\"\n",
    "ERR_SCHEMA_VALIDATION = \"schema_validation_error\"\n",
    "\n",
    "\n",
    "def _get_response_text(completion: Any) -> str:\n",
    "    response_text = \"\"\n",
    "    if isinstance(completion, str):\n",
    "        response_text = completion\n",
    "    elif isinstance(completion, list) and completion:\n",
    "        if isinstance(completion[0], dict):\n",
    "            response_text = completion[0].get(\n",
    "                'content', completion[0].get('generated_text', ''))\n",
    "        elif isinstance(completion[0], str):\n",
    "            response_text = completion[0]\n",
    "    elif isinstance(completion, dict):\n",
    "        response_text = completion.get(\n",
    "            'content', completion.get('generated_text', ''))\n",
    "    else:\n",
    "        try:\n",
    "            response_text = str(completion)\n",
    "        except Exception:\n",
    "            response_text = \"\"\n",
    "            logger.debug(\n",
    "                \"Could not convert completion to string: %s\", type(completion))\n",
    "    return response_text\n",
    "\n",
    "\n",
    "def extract_sql(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    match = re.search(r\"<sql>(.*?)</sql>\", text, re.IGNORECASE | re.DOTALL)\n",
    "    if match:\n",
    "        sql = match.group(1).strip()\n",
    "        sql = re.sub(r\"^\\s*--.*?\\n\", \"\", sql, flags=re.MULTILINE)\n",
    "        sql = re.sub(r\"\\n--.*?\\s*$\", \"\", sql, flags=re.MULTILINE)\n",
    "        sql = re.sub(r\"/\\*.*?\\*/\", \"\", sql, flags=re.DOTALL)\n",
    "        return sql.strip()\n",
    "    else:\n",
    "        sql_keywords = [\"SELECT \", \"INSERT \", \"UPDATE \",\n",
    "                        \"DELETE \", \"CREATE \", \"ALTER \", \"DROP \", \"WITH \"]\n",
    "        text_upper = text.upper()\n",
    "        sql_start_index = -1\n",
    "        for keyword in sql_keywords:\n",
    "            idx = text_upper.find(keyword)\n",
    "            if idx != -1 and (sql_start_index == -1 or idx < sql_start_index):\n",
    "                sql_start_index = idx\n",
    "        if sql_start_index != -1:\n",
    "            potential_sql = text[sql_start_index:]\n",
    "            potential_sql = potential_sql.split(\"</sql>\", 1)[0]\n",
    "            potential_sql = potential_sql.split(\"</reasoning>\", 1)[0]\n",
    "            if \";\" in potential_sql:\n",
    "                potential_sql = potential_sql.split(\";\", 1)[0] + \";\"\n",
    "            logger.debug(\"Extracted SQL using fallback method.\")\n",
    "            return potential_sql.strip()\n",
    "        logger.debug(\n",
    "            \"Could not extract SQL using primary or fallback methods.\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_reasoning(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    match = re.search(r\"<reasoning>(.*?)</reasoning>\",\n",
    "                      text, re.IGNORECASE | re.DOTALL)\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "\n",
    "def calculate_sql_complexity(sql: str) -> float:\n",
    "    if not sql:\n",
    "        return 0.0\n",
    "    try:\n",
    "        sql_upper = sql.upper()\n",
    "        score = 1.0\n",
    "        score += sql_upper.count(\" JOIN \") * 0.6\n",
    "        score += sql_upper.count(\" UNION \") * 0.8 + sql_upper.count(\n",
    "            \" INTERSECT \") * 0.8 + sql_upper.count(\" EXCEPT \") * 0.8\n",
    "        score += sql_upper.count(\"(SELECT\") * 1.0\n",
    "        score += sql_upper.count(\" WITH \") * 0.8\n",
    "        if \" WHERE \" in sql_upper:\n",
    "            score += 0.2\n",
    "        if \" GROUP BY \" in sql_upper:\n",
    "            score += 0.5\n",
    "        if \" HAVING \" in sql_upper:\n",
    "            score += 0.7\n",
    "        if \" ORDER BY \" in sql_upper:\n",
    "            score += 0.3\n",
    "        if \" LIMIT \" in sql_upper:\n",
    "            score += 0.1\n",
    "        agg_functions = [\"COUNT(\", \"SUM(\", \"AVG(\", \"MAX(\", \"MIN(\"]\n",
    "        score += sum(sql_upper.count(agg) for agg in agg_functions) * 0.3\n",
    "        score += sql_upper.count(\" DISTINCT \") * 0.3\n",
    "        score += sql_upper.count(\" CASE \") * 0.4\n",
    "        score += sql_upper.count(\" OVER(\") * 1.0\n",
    "        where_match = re.search(\n",
    "            r\" WHERE (.*?)(?: GROUP BY | ORDER BY | LIMIT | OFFSET |$)\", sql_upper, re.DOTALL)\n",
    "        if where_match:\n",
    "            where_clause = where_match.group(1)\n",
    "            score += where_clause.count(\" AND \") * \\\n",
    "                0.15 + where_clause.count(\" OR \") * 0.20\n",
    "            score += where_clause.count(\" IN \") * \\\n",
    "                0.2 + where_clause.count(\" LIKE \") * 0.1\n",
    "            score += where_clause.count(\" BETWEEN \") * \\\n",
    "                0.2 + where_clause.count(\" EXISTS \") * 0.3\n",
    "        return max(0.0, score)\n",
    "    except Exception as e:\n",
    "        logger.warning(\n",
    "            f\"Error calculating complexity for '{sql[:50]}...': {e}\")\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "def identify_sql_statement_type(sql: str) -> str:\n",
    "    if not sql:\n",
    "        return \"UNKNOWN\"\n",
    "    clean_sql = re.sub(r'--.*?$', '', sql, flags=re.MULTILINE).strip()\n",
    "    clean_sql = re.sub(r'/\\*.*?\\*/', '', clean_sql, flags=re.DOTALL).strip()\n",
    "    if not clean_sql:\n",
    "        return \"UNKNOWN\"\n",
    "    first_word = clean_sql.split(None, 1)[0].upper()\n",
    "\n",
    "    if first_word == \"SELECT\":\n",
    "        return \"SELECT\"\n",
    "    if first_word == \"INSERT\":\n",
    "        return \"INSERT\"\n",
    "    if first_word == \"UPDATE\":\n",
    "        return \"UPDATE\"\n",
    "    if first_word == \"DELETE\":\n",
    "        return \"DELETE\"\n",
    "    if first_word == \"CREATE\":\n",
    "        if re.search(r\"CREATE\\s+(TABLE|VIEW|INDEX)\", clean_sql[:30], re.IGNORECASE):\n",
    "            second_word = clean_sql.split(None, 2)[1].upper() if len(\n",
    "                clean_sql.split()) > 1 else \"\"\n",
    "            if second_word == \"TABLE\":\n",
    "                return \"CREATE_TABLE\"\n",
    "            if second_word == \"VIEW\":\n",
    "                return \"CREATE_VIEW\"\n",
    "            if second_word == \"INDEX\":\n",
    "                return \"CREATE_INDEX\"\n",
    "        return \"CREATE_OTHER\"\n",
    "    if first_word == \"DROP\":\n",
    "        return \"DROP\"\n",
    "    if first_word == \"ALTER\":\n",
    "        return \"ALTER\"\n",
    "    if first_word == \"WITH\":\n",
    "        match = re.search(r'\\)\\s*(SELECT|INSERT|UPDATE|DELETE)',\n",
    "                          clean_sql, re.IGNORECASE | re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "        return \"WITH_UNKNOWN\"\n",
    "    return \"OTHER\"\n",
    "\n",
    "\n",
    "def list_all_tables(conn: sqlite3.Connection) -> List[str]:\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\n",
    "            \"SELECT name FROM sqlite_master WHERE type IN ('table', 'view');\")\n",
    "        return [row[0] for row in cursor.fetchall()]\n",
    "    except sqlite3.Error as e:\n",
    "        logger.error(f\"Error listing tables/views: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_table_schema(conn: sqlite3.Connection, table_name: str) -> List[Tuple]:\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"PRAGMA table_info('{table_name}');\")\n",
    "        return cursor.fetchall()\n",
    "    except sqlite3.Error as e:\n",
    "        logger.warning(f\"Error getting schema for table {table_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def check_table_exists(conn: sqlite3.Connection, table_name: str) -> Tuple[bool, bool, Optional[str]]:\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\n",
    "            \"SELECT name FROM sqlite_master WHERE type IN ('table', 'view') AND name=?;\", (table_name,))\n",
    "        exact_match = cursor.fetchone()\n",
    "        if exact_match:\n",
    "            return True, True, table_name\n",
    "\n",
    "        cursor.execute(\n",
    "            \"SELECT name FROM sqlite_master WHERE type IN ('table', 'view') AND lower(name)=lower(?);\", (table_name,))\n",
    "        insensitive_match = cursor.fetchone()\n",
    "        if insensitive_match:\n",
    "            return False, True, insensitive_match[0]\n",
    "\n",
    "        return False, False, None\n",
    "    except sqlite3.Error as e:\n",
    "        logger.warning(\n",
    "            f\"Error checking existence for table/view {table_name}: {e}\")\n",
    "        return False, False, None\n",
    "\n",
    "\n",
    "def get_column_names(conn: sqlite3.Connection, table_name: str) -> List[str]:\n",
    "    schema = get_table_schema(conn, table_name)\n",
    "    return [col[1] for col in schema]\n",
    "\n",
    "\n",
    "def extract_tables_from_query(sql: str) -> Set[str]:\n",
    "    tables = set()\n",
    "    if not sql:\n",
    "        return tables\n",
    "    try:\n",
    "        parsed_expression = parse(sql, read=\"sqlite\")\n",
    "\n",
    "        if isinstance(parsed_expression, list):\n",
    "            for expr in parsed_expression:\n",
    "                if hasattr(expr, 'find_all'):\n",
    "                    for node in expr.find_all():\n",
    "                        if hasattr(node, 'name') and hasattr(node, 'is_table') and node.is_table:\n",
    "                            table_name = node.name\n",
    "                            if table_name:\n",
    "                                tables.add(table_name)\n",
    "        else:\n",
    "            for node in parsed_expression.find_all():\n",
    "                if hasattr(node, 'name') and hasattr(node, 'is_table') and node.is_table:\n",
    "                    table_name = node.name\n",
    "                    if table_name:\n",
    "                        tables.add(table_name)\n",
    "    except ParseError as e:\n",
    "        logger.warning(\n",
    "            f\"sqlglot failed to parse for table extraction: {e}. Falling back to regex.\")\n",
    "        pattern = r'(?:FROM|JOIN)\\s+([`\"\\[]?\\w+[`\"\\]]?)(?:\\s+(?:AS\\s+)?(\\w+))?'\n",
    "        for match in re.finditer(pattern, sql, re.IGNORECASE):\n",
    "            table = match.group(1).strip('`\"[]')\n",
    "            tables.add(table)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during table extraction: {e}\")\n",
    "\n",
    "    return tables\n",
    "\n",
    "\n",
    "def convert_sql_to_sqlite(sql: str, source_dialect: str = \"mysql\") -> Optional[str]:\n",
    "    if not sql or not sql.strip():\n",
    "        return sql\n",
    "    try:\n",
    "        if DEBUG_MODE:\n",
    "            logger.debug(\n",
    "                f\"Converting SQL from {source_dialect} to sqlite: {sql[:150]}...\")\n",
    "        if source_dialect == \"postgresql\":\n",
    "            try:\n",
    "                converted = transpile(sql, read=\"postgres\", write=\"sqlite\")\n",
    "            except ParseError as pg_err:\n",
    "                logger.warning(\n",
    "                    f\"PostgreSQL parse error: {pg_err}, trying fallback conversion...\")\n",
    "                modified_sql = sql\n",
    "                modified_sql = re.sub(\n",
    "                    r'(\\w+)::\\w+', r'CAST(\\1 AS TEXT)', modified_sql)\n",
    "                modified_sql = re.sub(\n",
    "                    r'\\s+RETURNING\\s+.*?$', '', modified_sql, flags=re.IGNORECASE)\n",
    "                try:\n",
    "                    converted = transpile(\n",
    "                        modified_sql, read=\"postgres\", write=\"sqlite\")\n",
    "                except ParseError:\n",
    "                    logger.warning(\"Falling back to generic SQL parsing...\")\n",
    "                    converted = transpile(\n",
    "                        modified_sql, read=\"generic\", write=\"sqlite\")\n",
    "        else:\n",
    "            converted = transpile(sql, read=source_dialect, write=\"sqlite\")\n",
    "        if converted and isinstance(converted, list) and converted[0]:\n",
    "            if DEBUG_MODE:\n",
    "                logger.debug(f\"Converted SQL: {converted[0][:150]}...\")\n",
    "            return converted[0]\n",
    "        else:\n",
    "            logger.warning(\n",
    "                f\"sqlglot transpile returned empty result for: {sql[:100]}...\")\n",
    "            return sql\n",
    "    except ParseError as e:\n",
    "        logger.warning(\n",
    "            f\"sqlglot ParseError during conversion: {e}. SQL: {sql[:150]}...\")\n",
    "        if 'AUTO_INCREMENT' in sql.upper():\n",
    "            modified_sql = re.sub(\n",
    "                r'AUTO_INCREMENT', 'AUTOINCREMENT', sql, flags=re.IGNORECASE)\n",
    "            modified_sql = re.sub(r'(\\w+)\\s+(?:INT|INTEGER)\\s+PRIMARY\\s+KEY\\s+AUTOINCREMENT',\n",
    "                                  r'\\1 INTEGER PRIMARY KEY AUTOINCREMENT', modified_sql, flags=re.IGNORECASE)\n",
    "            logger.debug(\"Applied manual AUTO_INCREMENT fix attempt.\")\n",
    "            return modified_sql\n",
    "        return sql\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Unexpected error during sqlglot conversion: {e}\", exc_info=DEBUG_MODE)\n",
    "        return sql\n",
    "\n",
    "\n",
    "def fix_case_sensitivity_in_sql(conn: sqlite3.Connection, sql: str) -> str:\n",
    "    if not sql:\n",
    "        return sql\n",
    "\n",
    "    corrected_sql = sql\n",
    "    all_db_tables = list_all_tables(conn)\n",
    "    if not all_db_tables:\n",
    "        return sql\n",
    "\n",
    "    table_case_map = {t.lower(): t for t in all_db_tables}\n",
    "\n",
    "    referenced_tables = extract_tables_from_query(corrected_sql)\n",
    "    needs_table_fix = False\n",
    "    for table in referenced_tables:\n",
    "        table_lower = table.lower()\n",
    "        if table not in table_case_map.values() and table_lower in table_case_map:\n",
    "            correct_case_table = table_case_map[table_lower]\n",
    "            logger.debug(\n",
    "                f\"Case Fix: Replacing table '{table}' with '{correct_case_table}'\")\n",
    "            corrected_sql = re.sub(r'\\b' + re.escape(table) + r'\\b',\n",
    "                                   correct_case_table, corrected_sql, flags=re.IGNORECASE)\n",
    "            needs_table_fix = True\n",
    "\n",
    "    if needs_table_fix:\n",
    "        logger.debug(\n",
    "            f\"SQL after table case correction: {corrected_sql[:150]}...\")\n",
    "\n",
    "    current_referenced_tables = extract_tables_from_query(corrected_sql)\n",
    "    needs_column_fix = False\n",
    "\n",
    "    try:\n",
    "        parsed_exp = parse(corrected_sql, read=\"sqlite\")\n",
    "\n",
    "        if isinstance(parsed_exp, list):\n",
    "            all_col_refs = []\n",
    "            for expr in parsed_exp:\n",
    "                if hasattr(expr, 'find_all'):\n",
    "                    all_col_refs.extend(expr.find_all(Column))\n",
    "        else:\n",
    "            all_col_refs = parsed_exp.find_all(Column)\n",
    "\n",
    "        for col_exp in all_col_refs:\n",
    "            col_name = col_exp.name\n",
    "            table_alias_or_name = col_exp.table\n",
    "\n",
    "            target_table = None\n",
    "            if table_alias_or_name:\n",
    "                if table_alias_or_name.lower() in table_case_map:\n",
    "                    target_table = table_case_map[table_alias_or_name.lower()]\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if target_table:\n",
    "                db_columns = get_column_names(conn, target_table)\n",
    "                col_case_map = {c.lower(): c for c in db_columns}\n",
    "                if col_name not in db_columns and col_name.lower() in col_case_map:\n",
    "                    correct_case_col = col_case_map[col_name.lower()]\n",
    "                    logger.debug(\n",
    "                        f\"Case Fix: Replacing column '{table_alias_or_name}.{col_name}' with '{table_alias_or_name}.{correct_case_col}'\")\n",
    "                    pattern = r'\\b' + \\\n",
    "                        re.escape(table_alias_or_name) + \\\n",
    "                        r'\\s*\\.\\s*' + re.escape(col_name) + r'\\b'\n",
    "                    replacement = f\"{table_alias_or_name}.{correct_case_col}\"\n",
    "                    corrected_sql = re.sub(\n",
    "                        pattern, replacement, corrected_sql, flags=re.IGNORECASE)\n",
    "                    needs_column_fix = True\n",
    "\n",
    "            elif not table_alias_or_name:\n",
    "                possible_corrections = []\n",
    "                for ref_table_name_lower in current_referenced_tables:\n",
    "                    actual_ref_table = table_case_map.get(\n",
    "                        ref_table_name_lower.lower())\n",
    "                    if actual_ref_table:\n",
    "                        db_columns = get_column_names(conn, actual_ref_table)\n",
    "                        col_case_map = {c.lower(): c for c in db_columns}\n",
    "                        if col_name not in db_columns and col_name.lower() in col_case_map:\n",
    "                            possible_corrections.append(\n",
    "                                col_case_map[col_name.lower()])\n",
    "\n",
    "                if len(possible_corrections) == 1:\n",
    "                    correct_case_col = possible_corrections[0]\n",
    "                    logger.debug(\n",
    "                        f\"Case Fix: Replacing unqualified column '{col_name}' with '{correct_case_col}'\")\n",
    "                    pattern = r'(?<![\\w\\.])\\b' + re.escape(col_name) + r'\\b'\n",
    "                    corrected_sql = re.sub(\n",
    "                        pattern, correct_case_col, corrected_sql, flags=re.IGNORECASE)\n",
    "                    needs_column_fix = True\n",
    "                elif len(possible_corrections) > 1:\n",
    "                    logger.warning(\n",
    "                        f\"Ambiguous case correction for unqualified column '{col_name}'. Found in multiple tables. Skipping.\")\n",
    "\n",
    "    except ParseError as e:\n",
    "        logger.warning(\n",
    "            f\"sqlglot failed to parse for column case fixing: {e}. Column fix might be incomplete.\")\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Unexpected error during column case fixing: {e}\", exc_info=DEBUG_MODE)\n",
    "\n",
    "    if needs_column_fix:\n",
    "        logger.debug(\n",
    "            f\"SQL after column case correction: {corrected_sql[:150]}...\")\n",
    "\n",
    "    return corrected_sql\n",
    "\n",
    "\n",
    "def fix_ambiguous_columns(sql: str, conn: Optional[sqlite3.Connection] = None) -> str:\n",
    "    if \" JOIN \" not in sql.upper():\n",
    "        return sql\n",
    "\n",
    "    try:\n",
    "        parsed_exp = parse(sql, read=\"sqlite\")\n",
    "        common_ambiguous = {'id', 'name', 'date', 'code', 'created_at', 'updated_at',\n",
    "                            'description', 'status', 'type', 'price', 'quantity', 'amount'}\n",
    "        first_table_alias = None\n",
    "\n",
    "        if isinstance(parsed_exp, list):\n",
    "            tables = []\n",
    "            for expr in parsed_exp:\n",
    "                if hasattr(expr, 'find_all'):\n",
    "                    for node in expr.find_all():\n",
    "                        if hasattr(node, 'name') and hasattr(node, 'is_table') and node.is_table:\n",
    "                            tables.append(node)\n",
    "        else:\n",
    "            tables = [node for node in parsed_exp.find_all()\n",
    "                      if hasattr(node, 'name') and hasattr(node, 'is_table') and node.is_table]\n",
    "\n",
    "        if tables:\n",
    "            first_table_alias = tables[0].alias_or_name\n",
    "\n",
    "        if not first_table_alias:\n",
    "            return sql\n",
    "\n",
    "        fixed_sql = sql\n",
    "        modified = False\n",
    "\n",
    "        if isinstance(parsed_exp, list):\n",
    "            all_col_refs = []\n",
    "            for expr in parsed_exp:\n",
    "                if hasattr(expr, 'find_all'):\n",
    "                    all_col_refs.extend(expr.find_all(Column))\n",
    "        else:\n",
    "            all_col_refs = parsed_exp.find_all(Column)\n",
    "\n",
    "        for col_exp in all_col_refs:\n",
    "            if not col_exp.table and col_exp.name.lower() in common_ambiguous:\n",
    "                logger.debug(\n",
    "                    f\"Ambiguity Fix: Qualifying '{col_exp.name}' with '{first_table_alias}'\")\n",
    "                pattern = r'(?<![\\w\\.])\\b' + re.escape(col_exp.name) + r'\\b'\n",
    "                replacement = f\"{first_table_alias}.{col_exp.name}\"\n",
    "                fixed_sql = re.sub(pattern, replacement,\n",
    "                                   fixed_sql, flags=re.IGNORECASE)\n",
    "                modified = True\n",
    "\n",
    "        if modified:\n",
    "            logger.debug(\n",
    "                f\"SQL after ambiguity fix attempt: {fixed_sql[:150]}...\")\n",
    "            return fixed_sql\n",
    "        else:\n",
    "            return sql\n",
    "\n",
    "    except ParseError:\n",
    "        logger.warning(\n",
    "            \"Failed to parse SQL for ambiguity fixing. Returning original.\")\n",
    "        return sql\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Error during ambiguity fixing: {e}\", exc_info=DEBUG_MODE)\n",
    "        return sql\n",
    "\n",
    "\n",
    "def categorize_sql_error(error_msg: str) -> Tuple[str, float]:\n",
    "    if not error_msg:\n",
    "        return ERR_OTHER, 0.0\n",
    "    error_lower = error_msg.lower()\n",
    "    if DEBUG_MODE:\n",
    "        logger.debug(f\"Categorizing SQL error: {error_msg}\")\n",
    "\n",
    "    if \"syntax error\" in error_lower:\n",
    "        return ERR_SYNTAX, 0.0\n",
    "    if \"no such table\" in error_lower:\n",
    "        return ERR_MISSING_TABLE, 0.0\n",
    "    if \"no such column\" in error_lower:\n",
    "        return ERR_MISSING_COLUMN, 0.1\n",
    "    if \"ambiguous column\" in error_lower:\n",
    "        return ERR_AMBIGUOUS_COLUMN, 0.2\n",
    "    if \"datatype mismatch\" in error_lower:\n",
    "        return ERR_TYPE_MISMATCH, 0.15\n",
    "    if \"constraint failed\" in error_lower or \"constraint violation\" in error_lower:\n",
    "        return ERR_CONSTRAINT, 0.1\n",
    "    if \"no such function\" in error_lower:\n",
    "        return ERR_FUNCTION, 0.05\n",
    "    if \"too many terms in compound select\" in error_lower:\n",
    "        return ERR_SYNTAX, 0.0\n",
    "    if \"subquery returned more than 1 row\" in error_lower:\n",
    "        return ERR_EXECUTION, 0.1\n",
    "\n",
    "    return ERR_OTHER, 0.0\n",
    "\n",
    "\n",
    "def strict_format_reward_func(prompts, completions, references=None, **kwargs) -> list[float]:\n",
    "    strict_pattern = r\"<reasoning>(.+?)</reasoning>\\s*<sql>(.+?)</sql>\"\n",
    "    base_reward = REWARD_WEIGHTS.get(\"format\", 1.0)\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        response_text = _get_response_text(completion)\n",
    "        match = re.search(strict_pattern, response_text,\n",
    "                          re.IGNORECASE | re.DOTALL)\n",
    "        rewards.append(base_reward if match else 0.0)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def soft_format_reward_func(prompts, completions, references=None, **kwargs) -> list[float]:\n",
    "    soft_pattern = r\"<reasoning>(.*?)</reasoning>\\s*<sql>(.*?)</sql>\"\n",
    "    base_reward = REWARD_WEIGHTS.get(\"format\", 1.0)\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        response_text = _get_response_text(completion)\n",
    "        match = re.search(soft_pattern, response_text,\n",
    "                          re.IGNORECASE | re.DOTALL)\n",
    "        rewards.append(base_reward if match else 0.0)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def extract_tables_columns(sql_context: str) -> tuple[set[str], set[str]]:\n",
    "    tables = set()\n",
    "    columns = set()\n",
    "    if not sql_context:\n",
    "        return tables, columns\n",
    "\n",
    "    create_table_pattern = r\"CREATE\\s+TABLE\\s+(?:IF\\s+NOT\\s+EXISTS\\s+)?(?:[`\\\"\\[]?(\\w+)[`\\\"\\]]?)\\s*\\((.*?)\\);\"\n",
    "    create_view_pattern = r\"CREATE\\s+VIEW\\s+(?:[`\\\"\\[]?(\\w+)[`\\\"\\]]?)\\s+AS\"\n",
    "    column_pattern = r\"^\\s*([`\\\"\\[]?\\w+[`\\\"\\]]?)\"\n",
    "\n",
    "    try:\n",
    "        statements = sqlparse.split(sql_context)\n",
    "        for stmt in statements:\n",
    "            stmt_clean = stmt.strip()\n",
    "            table_match = re.search(\n",
    "                create_table_pattern, stmt_clean, re.IGNORECASE | re.DOTALL | re.MULTILINE)\n",
    "            if table_match:\n",
    "                table_name = table_match.group(1).lower()\n",
    "                tables.add(table_name)\n",
    "                cols_text = table_match.group(2)\n",
    "                for part in re.split(r',(?![^\\(]*\\))', cols_text):\n",
    "                    col_match = re.match(column_pattern, part.strip())\n",
    "                    if col_match:\n",
    "                        columns.add(col_match.group(1).strip('`\"[]').lower())\n",
    "            view_match = re.search(create_view_pattern,\n",
    "                                   stmt_clean, re.IGNORECASE)\n",
    "            if view_match:\n",
    "                view_name = view_match.group(1).lower()\n",
    "                tables.add(view_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not parse schema elements from context: {e}\")\n",
    "\n",
    "    return tables, columns\n",
    "\n",
    "\n",
    "def reasoning_quality_reward(prompts, completions, references=None, **kwargs) -> list[float]:\n",
    "    rewards = []\n",
    "    schema_cache = {}\n",
    "\n",
    "    for i, completion in enumerate(completions):\n",
    "        response_text = _get_response_text(completion)\n",
    "        reasoning = extract_reasoning(response_text)\n",
    "        reward_components = {}\n",
    "\n",
    "        if not reasoning:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "\n",
    "        reasoning_lower = reasoning.lower()\n",
    "        words = reasoning.split()\n",
    "        lines = [line for line in reasoning.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        len_score = 0.0\n",
    "        if len(words) >= 50:\n",
    "            len_score = 0.20\n",
    "        elif len(words) >= 25:\n",
    "            len_score = 0.15\n",
    "        elif len(words) >= 10:\n",
    "            len_score = 0.10\n",
    "        reward_components['length'] = len_score\n",
    "\n",
    "        sql_terms = [\"table\", \"column\", \"join\", \"select\", \"where\",\n",
    "                     \"group by\", \"order by\", \"filter\", \"aggregate\", \"schema\", \"database\"]\n",
    "        term_count = sum(1 for term in sql_terms if term in reasoning_lower)\n",
    "        term_score = min(0.20, term_count * 0.03)\n",
    "        reward_components['terms'] = term_score\n",
    "\n",
    "        structure_score = 0.0\n",
    "        if len(lines) >= 3:\n",
    "            structure_score = 0.15\n",
    "        elif len(lines) >= 2:\n",
    "            structure_score = 0.10\n",
    "        reward_components['structure'] = structure_score\n",
    "\n",
    "        step_score = 0.0\n",
    "        if re.search(r'(step 1|first|start|initial|begin)', reasoning_lower) and \\\n",
    "           re.search(r'(step 2|next|then|second|final|last|subsequent)', reasoning_lower):\n",
    "            step_score = 0.15\n",
    "        reward_components['steps'] = step_score\n",
    "\n",
    "        schema_mention_score = 0.0\n",
    "        sql_context = None\n",
    "        try:\n",
    "            if references and i < len(references) and references[i] and isinstance(references[i], list) and references[i][0]:\n",
    "                sql_context = references[i][0].get('sql_context')\n",
    "        except IndexError:\n",
    "            logger.warning(f\"IndexError accessing references at index {i}\")\n",
    "\n",
    "        if sql_context:\n",
    "            if i not in schema_cache:\n",
    "                schema_cache[i] = extract_tables_columns(\n",
    "                    sql_context) if isinstance(sql_context, str) else (set(), set())\n",
    "            tables, columns = schema_cache[i]\n",
    "\n",
    "            if tables or columns:\n",
    "                mentioned_tables = sum(1 for t in tables if re.search(\n",
    "                    r'\\b' + re.escape(t) + r'\\b', reasoning_lower))\n",
    "                mentioned_cols = sum(1 for c in columns if re.search(\n",
    "                    r'\\b' + re.escape(c) + r'\\b', reasoning_lower))\n",
    "                total_mentions = mentioned_tables + mentioned_cols\n",
    "                schema_mention_score = min(0.30, total_mentions * 0.05)\n",
    "        reward_components['schema'] = schema_mention_score\n",
    "\n",
    "        total_unscaled_reward = sum(reward_components.values())\n",
    "        final_reward = min(1.0, total_unscaled_reward) * \\\n",
    "            REWARD_WEIGHTS.get(\"reasoning\", 0.7)\n",
    "        rewards.append(final_reward)\n",
    "        if DEBUG_MODE:\n",
    "            logger.debug(\n",
    "                f\"Reasoning Scores (Comp {i}): {reward_components} -> Total Raw: {total_unscaled_reward:.3f} -> Final: {final_reward:.3f}\")\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def complexity_reward(prompts, completions, references, **kwargs) -> list[float]:\n",
    "    rewards = []\n",
    "    base_weight = REWARD_WEIGHTS.get(\"complexity\", 0.6)\n",
    "\n",
    "    for i, completion in enumerate(completions):\n",
    "        response_text = _get_response_text(completion)\n",
    "        gen_sql = extract_sql(response_text)\n",
    "        reward = 0.0\n",
    "\n",
    "        gold_sql = \"\"\n",
    "        try:\n",
    "            if references and i < len(references) and references[i] and isinstance(references[i], list) and references[i][0]:\n",
    "                gold_sql = references[i][0].get('gold_sql', '')\n",
    "        except IndexError:\n",
    "            logger.warning(f\"IndexError accessing references at index {i}\")\n",
    "\n",
    "        if not gen_sql:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            gen_complexity = calculate_sql_complexity(gen_sql)\n",
    "\n",
    "            if not gold_sql:\n",
    "                reward = (0.4 if 1.5 <= gen_complexity <=\n",
    "                          8.0 else 0.1) * base_weight\n",
    "                if DEBUG_MODE:\n",
    "                    logger.debug(\n",
    "                        f\"Complexity (Comp {i}): No Gold SQL. Gen={gen_complexity:.2f}. Reward={reward:.3f}\")\n",
    "            else:\n",
    "                gold_complexity = calculate_sql_complexity(gold_sql)\n",
    "                if gold_complexity < 0.1:\n",
    "                    rel_score = 1.0 if gen_complexity < 0.1 else 0.0\n",
    "                else:\n",
    "                    ratio = max(\n",
    "                        1e-3, min(gen_complexity / gold_complexity, 1e3))\n",
    "                    log_ratio = torch.log(torch.tensor(ratio))\n",
    "                    rel_score = torch.exp(-0.5 * (log_ratio**2)).item()\n",
    "                reward = rel_score * base_weight\n",
    "                if DEBUG_MODE:\n",
    "                    logger.debug(\n",
    "                        f\"Complexity (Comp {i}): Gen={gen_complexity:.2f}, Gold={gold_complexity:.2f}, Ratio={ratio:.2f}, Score={rel_score:.3f}, Reward={reward:.3f}\")\n",
    "\n",
    "            rewards.append(max(0.0, reward))\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error in complexity reward calculation: {e}\")\n",
    "            rewards.append(0.0)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def dump_database_schema(conn):\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        tables = list_all_tables(conn)\n",
    "\n",
    "        schema_info = {}\n",
    "\n",
    "        for table in tables:\n",
    "            cursor.execute(f\"PRAGMA table_info({table})\")\n",
    "            columns = cursor.fetchall()\n",
    "\n",
    "            column_info = []\n",
    "            for col in columns:\n",
    "                col_id, name, col_type, not_null, default_val, is_pk = col\n",
    "                col_desc = f\"{name} ({col_type})\"\n",
    "                if is_pk:\n",
    "                    col_desc += \" PRIMARY KEY\"\n",
    "                if not_null:\n",
    "                    col_desc += \" NOT NULL\"\n",
    "                if default_val is not None:\n",
    "                    col_desc += f\" DEFAULT {default_val}\"\n",
    "                column_info.append(col_desc)\n",
    "\n",
    "            schema_info[table] = column_info\n",
    "\n",
    "            cursor.execute(f\"PRAGMA index_list({table})\")\n",
    "            indexes = cursor.fetchall()\n",
    "            if indexes:\n",
    "                schema_info[f\"{table}_indexes\"] = []\n",
    "                for idx in indexes:\n",
    "                    idx_name = idx[1]\n",
    "                    cursor.execute(f\"PRAGMA index_info({idx_name})\")\n",
    "                    idx_columns = cursor.fetchall()\n",
    "                    idx_cols = [info[2] for info in idx_columns]\n",
    "                    schema_info[f\"{table}_indexes\"].append(\n",
    "                        f\"{idx_name} ({', '.join(idx_cols)})\")\n",
    "\n",
    "        return schema_info\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error dumping database schema: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "def execute_query_reward_func(prompts, completions, references, **kwargs) -> list[float]:\n",
    "    rewards = []\n",
    "\n",
    "    for i, completion in enumerate(completions):\n",
    "        response_text = _get_response_text(completion)\n",
    "        gen_sql = extract_sql(response_text)\n",
    "\n",
    "        gold_sql = \"\"\n",
    "        sql_context = \"\"\n",
    "        try:\n",
    "            if references and i < len(references) and references[i] and isinstance(references[i], list) and references[i][0]:\n",
    "                gold_sql = references[i][0].get('gold_sql', '')\n",
    "                sql_context = references[i][0].get('sql_context', '')\n",
    "\n",
    "                if DEBUG_MODE:\n",
    "                    logger.debug(\n",
    "                        f\"Reference {i}: Gold SQL = {gold_sql[:100]}...\")\n",
    "                    logger.debug(\n",
    "                        f\"Reference {i}: Context SQL  = {sql_context}\")\n",
    "        except IndexError:\n",
    "            logger.warning(f\"IndexError accessing references at index {i}\")\n",
    "\n",
    "        reward = 0.0\n",
    "\n",
    "        if not gen_sql or not gold_sql or not sql_context:\n",
    "            logger.warning(\n",
    "                f\"Missing SQL data for completion {i}: gen_sql={bool(gen_sql)}, gold_sql={bool(gold_sql)}, sql_context={bool(sql_context)}\")\n",
    "            rewards.append(reward)\n",
    "            continue\n",
    "\n",
    "        gold_type = identify_sql_statement_type(gold_sql)\n",
    "        gen_type = identify_sql_statement_type(gen_sql)\n",
    "\n",
    "        if gold_type == gen_type:\n",
    "            reward += 0.1 * REWARD_WEIGHTS[\"sql_correctness\"]\n",
    "\n",
    "        if DEBUG_MODE:\n",
    "            logger.debug(f\"Gold SQL type: {gold_type}\")\n",
    "            logger.debug(f\"Generated SQL type: {gen_type}\")\n",
    "\n",
    "        conn = None\n",
    "        temp_db_file = None\n",
    "        try:\n",
    "            temp_db_file = tempfile.NamedTemporaryFile(delete=False).name\n",
    "            conn = sqlite3.connect(temp_db_file, timeout=5)\n",
    "            conn.isolation_level = None\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            create_table_statements = []\n",
    "            create_view_statements = []\n",
    "            other_statements = []\n",
    "\n",
    "            for stmt in sqlparse.split(sql_context):\n",
    "                stmt = stmt.strip()\n",
    "                if not stmt:\n",
    "                    continue\n",
    "\n",
    "                stmt_upper = stmt.upper()\n",
    "                if stmt_upper.startswith('CREATE TABLE'):\n",
    "                    create_table_statements.append(stmt)\n",
    "                elif stmt_upper.startswith('CREATE VIEW'):\n",
    "                    create_view_statements.append(stmt)\n",
    "                else:\n",
    "                    other_statements.append(stmt)\n",
    "\n",
    "            if DEBUG_MODE:\n",
    "                logger.debug(f\"Found {len(create_table_statements)} CREATE TABLE statements, \"\n",
    "                             f\"{len(create_view_statements)} CREATE VIEW statements, and \"\n",
    "                             f\"{len(other_statements)} other statements\")\n",
    "\n",
    "            tables_created = []\n",
    "            for stmt in create_table_statements:\n",
    "                try:\n",
    "                    table_match = re.search(r'CREATE\\s+TABLE\\s+(?:IF\\s+NOT\\s+EXISTS\\s+)?([^\\s(]+)',\n",
    "                                            stmt, re.IGNORECASE)\n",
    "                    table_name = table_match.group(1).strip(\n",
    "                        '`\"[]') if table_match else \"unknown\"\n",
    "\n",
    "                    converted_stmt = convert_sql_to_sqlite(stmt)\n",
    "\n",
    "                    if DEBUG_MODE:\n",
    "                        logger.debug(\n",
    "                            f\"Creating table {table_name} with statement: {converted_stmt[:100]}...\")\n",
    "\n",
    "                    cursor.execute(converted_stmt)\n",
    "                    tables_created.append(table_name)\n",
    "\n",
    "                    exists_exact, exists_case_insensitive, correct_case = check_table_exists(\n",
    "                        conn, table_name)\n",
    "                    if exists_exact:\n",
    "                        if DEBUG_MODE:\n",
    "                            logger.debug(\n",
    "                                f\"Table {table_name} created successfully\")\n",
    "                            schema = get_table_schema(conn, table_name)\n",
    "                            logger.debug(f\"Schema for {table_name}: {schema}\")\n",
    "                    else:\n",
    "                        logger.warning(\n",
    "                            f\"Table {table_name} creation failed silently\")\n",
    "\n",
    "                except sqlite3.Error as e:\n",
    "                    logger.warning(f\"Error in CREATE TABLE statement: {e}\")\n",
    "                    logger.warning(\n",
    "                        f\"Table name: {table_name if 'table_name' in locals() else 'unknown'}\")\n",
    "                    logger.warning(f\"Original statement: {stmt[:200]}...\")\n",
    "                    logger.warning(f\"Converted statement: {converted_stmt[:200]}...\" if 'converted_stmt' in locals(\n",
    "                    ) else \"conversion failed\")\n",
    "\n",
    "            views_created = []\n",
    "            for stmt in create_view_statements:\n",
    "                try:\n",
    "                    view_match = re.search(\n",
    "                        r'CREATE\\s+VIEW\\s+([^\\s(]+)', stmt, re.IGNORECASE)\n",
    "                    view_name = view_match.group(1).strip(\n",
    "                        '`\"[]') if view_match else \"unknown\"\n",
    "\n",
    "                    converted_stmt = convert_sql_to_sqlite(stmt)\n",
    "\n",
    "                    if DEBUG_MODE:\n",
    "                        logger.debug(\n",
    "                            f\"Creating view {view_name} with statement: {converted_stmt[:100]}...\")\n",
    "\n",
    "                    cursor.execute(converted_stmt)\n",
    "                    views_created.append(view_name)\n",
    "\n",
    "                    exists_exact, exists_case_insensitive, correct_case = check_table_exists(\n",
    "                        conn, view_name)\n",
    "                    if exists_exact:\n",
    "                        if DEBUG_MODE:\n",
    "                            logger.debug(\n",
    "                                f\"View {view_name} created successfully\")\n",
    "                    else:\n",
    "                        logger.warning(\n",
    "                            f\"View {view_name} creation failed silently\")\n",
    "\n",
    "                except sqlite3.Error as e:\n",
    "                    logger.warning(f\"Error in CREATE VIEW statement: {e}\")\n",
    "                    logger.warning(\n",
    "                        f\"View name: {view_name if 'view_name' in locals() else 'unknown'}\")\n",
    "                    logger.warning(f\"Original statement: {stmt[:200]}...\")\n",
    "                    logger.warning(f\"Converted statement: {converted_stmt[:200]}...\" if 'converted_stmt' in locals(\n",
    "                    ) else \"conversion failed\")\n",
    "\n",
    "            for stmt in other_statements:\n",
    "                try:\n",
    "                    is_insert_like = stmt.upper().startswith(\n",
    "                        \"INSERT\") or \"INSERT INTO\" in stmt.upper()\n",
    "\n",
    "                    converted_stmt = convert_sql_to_sqlite(stmt)\n",
    "\n",
    "                    if DEBUG_MODE and is_insert_like:\n",
    "                        logger.debug(\n",
    "                            f\"Executing insert-like statement: {converted_stmt[:100]}...\")\n",
    "\n",
    "                    cursor.execute(converted_stmt)\n",
    "                except sqlite3.Error as e:\n",
    "                    logger.warning(f\"Error in non-CREATE statement: {e}\")\n",
    "                    logger.warning(f\"Statement causing error: {stmt[:200]}...\")\n",
    "\n",
    "            if DEBUG_MODE:\n",
    "                schema_info = dump_database_schema(conn)\n",
    "                logger.debug(f\"Database schema after setup: {schema_info}\")\n",
    "\n",
    "                all_tables = list_all_tables(conn)\n",
    "                logger.debug(f\"All tables in database: {all_tables}\")\n",
    "\n",
    "            referenced_tables = extract_tables_from_query(gen_sql)\n",
    "            if DEBUG_MODE:\n",
    "                logger.debug(\n",
    "                    f\"Tables referenced in generated query: {referenced_tables}\")\n",
    "\n",
    "                for table in referenced_tables:\n",
    "                    exists_exact, exists_case_insensitive, correct_case = check_table_exists(\n",
    "                        conn, table)\n",
    "                    if exists_exact:\n",
    "                        logger.debug(\n",
    "                            f\"Table '{table}' referenced in query exists exactly as specified\")\n",
    "                    elif exists_case_insensitive:\n",
    "                        logger.debug(\n",
    "                            f\"Table '{table}' exists but with different case: '{correct_case}'\")\n",
    "                    else:\n",
    "                        logger.debug(\n",
    "                            f\"Table '{table}' does not exist in any case form\")\n",
    "\n",
    "            existing_tables = list_all_tables(conn)\n",
    "            existing_tables_lower = [t.lower() for t in existing_tables]\n",
    "            missing_tables = [table for table in referenced_tables if table.lower(\n",
    "            ) not in existing_tables_lower]\n",
    "            case_mismatch_tables = [\n",
    "                table for table in referenced_tables if table not in existing_tables and table.lower() in existing_tables_lower]\n",
    "\n",
    "            if case_mismatch_tables:\n",
    "                logger.warning(\n",
    "                    f\"Case-mismatch in table references: {case_mismatch_tables}\")\n",
    "\n",
    "                case_mapping = {t.lower(): t for t in existing_tables}\n",
    "\n",
    "                for wrong_case in case_mismatch_tables:\n",
    "                    correct_case = case_mapping[wrong_case.lower()]\n",
    "                    logger.debug(\n",
    "                        f\"Fixing case: '{wrong_case}' → '{correct_case}'\")\n",
    "\n",
    "                    gen_sql = re.sub(r'\\b' + re.escape(wrong_case) + r'\\b',\n",
    "                                     correct_case,\n",
    "                                     gen_sql,\n",
    "                                     flags=re.IGNORECASE)\n",
    "\n",
    "                logger.debug(\n",
    "                    f\"Adjusted SQL with correct case: {gen_sql[:200]}...\")\n",
    "\n",
    "            if missing_tables:\n",
    "                logger.warning(\n",
    "                    f\"Tables genuinely missing (not just case mismatch): {missing_tables}\")\n",
    "\n",
    "            if gold_type == \"SELECT\" and gen_type == \"SELECT\":\n",
    "                try:\n",
    "                    fixed_gen_sql = fix_ambiguous_columns(gen_sql)\n",
    "\n",
    "                    if fixed_gen_sql != gen_sql:\n",
    "                        logger.debug(\n",
    "                            f\"Fixed ambiguous columns in generated SQL\")\n",
    "                        logger.debug(f\"Original SQL: {gen_sql[:200]}...\")\n",
    "                        logger.debug(f\"Fixed SQL: {fixed_gen_sql[:200]}...\")\n",
    "                        gen_sql = fixed_gen_sql\n",
    "\n",
    "                    converted_gold_sql = convert_sql_to_sqlite(gold_sql)\n",
    "                    logger.debug(\n",
    "                        f\"Executing gold SQL: {converted_gold_sql[:200]}...\")\n",
    "\n",
    "                    cursor.execute(converted_gold_sql)\n",
    "                    gold_columns = [\n",
    "                        desc[0] for desc in cursor.description] if cursor.description else []\n",
    "                    gold_result = cursor.fetchmany(1000)\n",
    "\n",
    "                    logger.debug(\n",
    "                        f\"Gold SQL execution successful, returned {len(gold_result)} rows\")\n",
    "                    if gold_result and len(gold_result) > 0:\n",
    "                        logger.debug(\n",
    "                            f\"First row of gold result: {gold_result[0]}\")\n",
    "\n",
    "                    gen_sql_fixed = fix_case_sensitivity_in_sql(conn, gen_sql)\n",
    "\n",
    "                    if gen_sql_fixed != gen_sql:\n",
    "                        logger.debug(\n",
    "                            f\"Fixed case sensitivity issues in generated SQL\")\n",
    "                        gen_sql = gen_sql_fixed\n",
    "\n",
    "                    converted_gen_sql = convert_sql_to_sqlite(gen_sql)\n",
    "                    logger.debug(\n",
    "                        f\"Executing generated SQL: {converted_gen_sql[:200]}...\")\n",
    "\n",
    "                    cursor.execute(converted_gen_sql)\n",
    "                    gen_columns = [\n",
    "                        desc[0] for desc in cursor.description] if cursor.description else []\n",
    "                    gen_result = cursor.fetchmany(1000)\n",
    "\n",
    "                    logger.debug(\n",
    "                        f\"Generated SQL execution successful, returned {len(gen_result)} rows\")\n",
    "                    if gen_result and len(gen_result) > 0:\n",
    "                        logger.debug(\n",
    "                            f\"First row of generated result: {gen_result[0]}\")\n",
    "\n",
    "                    base_reward = 0.3 * REWARD_WEIGHTS[\"sql_correctness\"]\n",
    "                    reward = base_reward\n",
    "\n",
    "                    gold_rows = set(tuple(row) for row in gold_result)\n",
    "                    gen_rows = set(tuple(row) for row in gen_result)\n",
    "\n",
    "                    if gold_rows == gen_rows and gold_columns == gen_columns:\n",
    "                        reward = REWARD_WEIGHTS[\"sql_correctness\"]\n",
    "                        logger.debug(f\"Results and columns match exactly!\")\n",
    "                    elif gold_rows and gen_rows:\n",
    "                        if gold_columns == gen_columns:\n",
    "                            intersection = len(\n",
    "                                gold_rows.intersection(gen_rows))\n",
    "                            union = len(gold_rows.union(gen_rows))\n",
    "                            jaccard = intersection / union if union > 0 else 0\n",
    "                        else:\n",
    "                            gold_cols_lower = [c.lower() for c in gold_columns]\n",
    "                            gen_cols_lower = [c.lower() for c in gen_columns]\n",
    "                            common_columns_indices = []\n",
    "\n",
    "                            for i, gold_col in enumerate(gold_cols_lower):\n",
    "                                if gold_col in gen_cols_lower:\n",
    "                                    j = gen_cols_lower.index(gold_col)\n",
    "                                    common_columns_indices.append((i, j))\n",
    "\n",
    "                            if common_columns_indices:\n",
    "                                gold_projected = [{i: row[i] for i, _ in common_columns_indices}\n",
    "                                                  for row in gold_result]\n",
    "                                gen_projected = [{j: row[j] for _, j in common_columns_indices}\n",
    "                                                 for row in gen_result]\n",
    "\n",
    "                                gold_proj_rows = {\n",
    "                                    tuple(sorted(d.items())) for d in gold_projected}\n",
    "                                gen_proj_rows = {\n",
    "                                    tuple(sorted(d.items())) for d in gen_projected}\n",
    "\n",
    "                                intersection = len(\n",
    "                                    gold_proj_rows.intersection(gen_proj_rows))\n",
    "                                union = len(\n",
    "                                    gold_proj_rows.union(gen_proj_rows))\n",
    "                                jaccard = intersection / union if union > 0 else 0\n",
    "\n",
    "                                if DEBUG_MODE:\n",
    "                                    logger.debug(\n",
    "                                        f\"Similarity calculated on {len(common_columns_indices)} common columns\")\n",
    "                            else:\n",
    "                                jaccard = 0.0\n",
    "\n",
    "                        row_count_ratio = min(len(gen_rows), len(gold_rows)) / max(\n",
    "                            len(gen_rows), len(gold_rows)) if max(len(gen_rows), len(gold_rows)) > 0 else 0\n",
    "\n",
    "                        col_similarity = 0.0\n",
    "                        if gold_columns and gen_columns:\n",
    "                            gold_cols_set = set(c.lower()\n",
    "                                                for c in gold_columns)\n",
    "                            gen_cols_set = set(c.lower() for c in gen_columns)\n",
    "                            col_intersection = len(\n",
    "                                gold_cols_set.intersection(gen_cols_set))\n",
    "                            col_union = len(gold_cols_set.union(gen_cols_set))\n",
    "                            col_similarity = col_intersection / col_union if col_union > 0 else 0\n",
    "\n",
    "                        data_accuracy = len(gold_rows.intersection(\n",
    "                            gen_rows)) / len(gold_rows) if gold_rows else 0\n",
    "\n",
    "                        content_similarity = (\n",
    "                            0.40 * jaccard +\n",
    "                            0.20 * row_count_ratio +\n",
    "                            0.25 * col_similarity +\n",
    "                            0.15 * data_accuracy\n",
    "                        )\n",
    "\n",
    "                        reward = REWARD_WEIGHTS[\"sql_correctness\"] * \\\n",
    "                            content_similarity\n",
    "\n",
    "                        if DEBUG_MODE:\n",
    "                            logger.debug(f\"Reward calculation: jaccard={jaccard:.3f}, row_ratio={row_count_ratio:.3f}, \" +\n",
    "                                         f\"col_sim={col_similarity:.3f}, data_acc={data_accuracy:.3f}, \" +\n",
    "                                         f\"content_sim={content_similarity:.3f}, final_reward={reward:.3f}\")\n",
    "\n",
    "                        if intersection > 0 and reward < 0.3 * REWARD_WEIGHTS[\"sql_correctness\"]:\n",
    "                            reward = 0.3 * REWARD_WEIGHTS[\"sql_correctness\"]\n",
    "\n",
    "                    if reward <= base_reward and gen_result is not None:\n",
    "                        reward = max(\n",
    "                            reward, 0.2 * REWARD_WEIGHTS[\"sql_correctness\"])\n",
    "\n",
    "                except sqlite3.Error as e:\n",
    "                    error_msg = str(e)\n",
    "                    error_type, partial_credit = categorize_sql_error(\n",
    "                        error_msg)\n",
    "\n",
    "                    if partial_credit > 0:\n",
    "                        reward = partial_credit * \\\n",
    "                            REWARD_WEIGHTS[\"sql_correctness\"]\n",
    "\n",
    "                    logger.warning(\n",
    "                        f\"Error executing SELECT statement ({error_type}): {error_msg}\")\n",
    "                    logger.warning(f\"Generated SQL: {gen_sql[:200]}...\")\n",
    "                    if 'converted_gen_sql' in locals():\n",
    "                        logger.warning(\n",
    "                            f\"Converted SQL: {converted_gen_sql[:200]}...\")\n",
    "\n",
    "            elif gen_type in [\"INSERT\", \"UPDATE\", \"DELETE\"]:\n",
    "                try:\n",
    "                    if \"JOIN\" in gen_sql.upper() and gen_type != \"SELECT\":\n",
    "                        logger.warning(\n",
    "                            f\"JOIN detected in {gen_type} statement - may cause issues\")\n",
    "\n",
    "                        if gen_type == \"INSERT\":\n",
    "                            table_match = re.search(\n",
    "                                r'INSERT\\s+INTO\\s+([^\\s(]+)', gen_sql, re.IGNORECASE)\n",
    "                            if table_match:\n",
    "                                main_table = table_match.group(1)\n",
    "                                logger.debug(\n",
    "                                    f\"Main table for INSERT: {main_table}\")\n",
    "                        elif gen_type == \"UPDATE\":\n",
    "                            table_match = re.search(\n",
    "                                r'UPDATE\\s+([^\\s(]+)', gen_sql, re.IGNORECASE)\n",
    "                            if table_match:\n",
    "                                main_table = table_match.group(1)\n",
    "                                logger.debug(\n",
    "                                    f\"Main table for UPDATE: {main_table}\")\n",
    "                        elif gen_type == \"DELETE\":\n",
    "                            table_match = re.search(\n",
    "                                r'DELETE\\s+FROM\\s+([^\\s(]+)', gen_sql, re.IGNORECASE)\n",
    "                            if table_match:\n",
    "                                main_table = table_match.group(1)\n",
    "                                logger.debug(\n",
    "                                    f\"Main table for DELETE: {main_table}\")\n",
    "\n",
    "                        if 'main_table' in locals():\n",
    "                            exists = check_table_exists(conn, main_table)\n",
    "                            logger.debug(\n",
    "                                f\"Main table '{main_table}' exists: {exists}\")\n",
    "\n",
    "                    gen_sql_fixed = fix_case_sensitivity_in_sql(conn, gen_sql)\n",
    "\n",
    "                    if gen_sql_fixed != gen_sql:\n",
    "                        logger.debug(\n",
    "                            f\"Fixed case sensitivity issues in DML statement\")\n",
    "                        gen_sql = gen_sql_fixed\n",
    "\n",
    "                    converted_gen_sql = convert_sql_to_sqlite(gen_sql)\n",
    "                    logger.debug(\n",
    "                        f\"Executing DML statement: {converted_gen_sql[:200]}...\")\n",
    "\n",
    "                    cursor.execute(converted_gen_sql)\n",
    "                    reward = 0.5 * REWARD_WEIGHTS[\"sql_correctness\"]\n",
    "\n",
    "                except sqlite3.Error as e:\n",
    "                    error_msg = str(e)\n",
    "                    logger.warning(\n",
    "                        f\"Error executing DML statement: {error_msg}\")\n",
    "                    logger.warning(f\"Generated SQL: {gen_sql[:200]}...\")\n",
    "\n",
    "                    if \"no such table\" in error_msg.lower():\n",
    "                        table_match = re.search(\n",
    "                            r\"no such table: (\\w+)\", error_msg, re.IGNORECASE)\n",
    "                        if table_match:\n",
    "                            missing_table = table_match.group(1)\n",
    "                            logger.debug(f\"Missing table: {missing_table}\")\n",
    "\n",
    "                            all_tables = list_all_tables(conn)\n",
    "                            logger.debug(f\"Available tables: {all_tables}\")\n",
    "\n",
    "                            case_mapping = {t.lower(): t for t in all_tables}\n",
    "\n",
    "                            if missing_table.lower() in case_mapping:\n",
    "                                correct_case = case_mapping[missing_table.lower(\n",
    "                                )]\n",
    "                                logger.debug(\n",
    "                                    f\"Case mismatch detected! '{missing_table}' vs '{correct_case}'\")\n",
    "\n",
    "                                corrected_sql = re.sub(r'\\b' + re.escape(missing_table) + r'\\b',\n",
    "                                                       correct_case,\n",
    "                                                       gen_sql,\n",
    "                                                       flags=re.IGNORECASE)\n",
    "\n",
    "                                logger.debug(\n",
    "                                    f\"Corrected SQL: {corrected_sql[:200]}...\")\n",
    "\n",
    "                                try:\n",
    "                                    converted_corrected = convert_sql_to_sqlite(\n",
    "                                        corrected_sql)\n",
    "                                    cursor.execute(converted_corrected)\n",
    "                                    reward = 0.4 * \\\n",
    "                                        REWARD_WEIGHTS[\"sql_correctness\"]\n",
    "                                    logger.debug(\n",
    "                                        f\"Execution successful after case correction!\")\n",
    "                                except sqlite3.Error as e2:\n",
    "                                    logger.warning(\n",
    "                                        f\"Still failed after case correction: {e2}\")\n",
    "                                    logger.debug(\n",
    "                                        f\"New error after case correction: {e2}\")\n",
    "                                    logger.debug(\n",
    "                                        f\"Converted corrected SQL: {converted_corrected[:200]}...\")\n",
    "\n",
    "            rewards.append(reward)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error in execution reward calculation: {e}\")\n",
    "            import traceback\n",
    "            logger.warning(f\"Stack trace: {traceback.format_exc()}\")\n",
    "            rewards.append(reward)\n",
    "        finally:\n",
    "            logger.debug(f\"Final reward for completion {i}: {reward}\")\n",
    "\n",
    "            if conn:\n",
    "                try:\n",
    "                    conn.close()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            try:\n",
    "                if temp_db_file and os.path.exists(temp_db_file):\n",
    "                    os.unlink(temp_db_file)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DEBUG_MODE = True\n",
    "    log_level = logging.DEBUG\n",
    "    logging.getLogger().setLevel(log_level)\n",
    "    for handler in logging.getLogger().handlers:\n",
    "        handler.setLevel(log_level)\n",
    "    logger.info(\"Running example usage...\")\n",
    "\n",
    "    prompts_example = [\"Show names of dogs older than 5 years.\"]\n",
    "    completions_example = [\n",
    "        \"<reasoning>Find dogs table. Filter by age > 5. Select name column.</reasoning>\\n<sql>SELECT name FROM dogs WHERE age > 5;</sql>\"\n",
    "    ]\n",
    "    references_example = [[{\n",
    "        \"gold_sql\": \"SELECT name FROM dogs WHERE age > 5 ORDER BY dog_id;\",\n",
    "        \"sql_context\": \"\"\"\n",
    "        CREATE TABLE dogs (dog_id INTEGER PRIMARY KEY, name TEXT, age INTEGER);\n",
    "        INSERT INTO dogs (name, age) VALUES ('Buddy', 7);\n",
    "        INSERT INTO dogs (name, age) VALUES ('Lucy', 4);\n",
    "        INSERT INTO dogs (name, age) VALUES ('Max', 8);\n",
    "        \"\"\",\n",
    "        \"question\": prompts_example[0]\n",
    "    }]]\n",
    "\n",
    "    print(\"\\n--- Testing execute_query_reward_func (Order Ignored) ---\")\n",
    "    exec_rewards_order_ignored = execute_query_reward_func(\n",
    "        prompts_example, completions_example, references_example,\n",
    "        source_dialect_dataset=\"mysql\",\n",
    "        source_dialect_generated=\"postgresql\",\n",
    "        order_matters=False,\n",
    "        validate_schema=True\n",
    "    )\n",
    "    print(f\"Execution Rewards (Order Ignored): {exec_rewards_order_ignored}\")\n",
    "\n",
    "    print(\"\\n--- Testing execute_query_reward_func (Order Matters) ---\")\n",
    "    exec_rewards_order_matters = execute_query_reward_func(\n",
    "        prompts_example, completions_example, references_example,\n",
    "        source_dialect_dataset=\"mysql\",\n",
    "        source_dialect_generated=\"postgresql\",\n",
    "        order_matters=True,\n",
    "        validate_schema=True\n",
    "    )\n",
    "    print(f\"Execution Rewards (Order Matters): {exec_rewards_order_matters}\")\n",
    "\n",
    "    print(\"\\n--- Testing Format Rewards ---\")\n",
    "    strict_format_rewards = strict_format_reward_func(\n",
    "        prompts_example, completions_example)\n",
    "    soft_format_rewards = soft_format_reward_func(\n",
    "        prompts_example, completions_example)\n",
    "    print(f\"Strict Format Rewards: {strict_format_rewards}\")\n",
    "    print(f\"Soft Format Rewards: {soft_format_rewards}\")\n",
    "\n",
    "    print(\"\\n--- Testing Complexity Reward ---\")\n",
    "    complexity_rewards = complexity_reward(\n",
    "        prompts_example, completions_example, references_example)\n",
    "    print(f\"Complexity Rewards: {complexity_rewards}\")\n",
    "\n",
    "    print(\"\\n--- Testing Reasoning Quality Reward ---\")\n",
    "    reasoning_rewards = reasoning_quality_reward(\n",
    "        prompts_example, completions_example, references_example)\n",
    "    print(f\"Reasoning Quality Rewards: {reasoning_rewards}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be45fd36",
   "metadata": {},
   "source": [
    "# Training and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b46b4c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import sqlparse\n",
    "import torch\n",
    "# from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from transformers import TrainerCallback\n",
    "# from sql_reward_utils import (\n",
    "#     soft_format_reward_func,\n",
    "#     strict_format_reward_func,\n",
    "#     execute_query_reward_func,\n",
    "#     complexity_reward,\n",
    "#     reasoning_quality_reward,\n",
    "#     REWARD_WEIGHTS\n",
    "# )\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "TRAIN_DATA_FILE = \"cleaned_train_queries.jsonl\"\n",
    "EVAL_DATA_FILE = \"cleaned_eval_queries.jsonl\"\n",
    "\n",
    "OUTPUT_DIR = \"outputs/sql_grpo\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def print_gpu_memory(step=\"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        max_allocated = torch.cuda.max_memory_allocated(0) / 1e9\n",
    "        print(f\"\\n--- GPU Memory at {step} ---\")\n",
    "        print(f\"Memory allocated: {allocated:.2f} GB\")\n",
    "        print(f\"Memory reserved: {reserved:.2f} GB\")\n",
    "        print(f\"Max memory allocated: {max_allocated:.2f} GB\")\n",
    "\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "LORA_RANK = 32\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION = 2\n",
    "NUM_GENERATIONS = 8\n",
    "MAX_STEPS = 250\n",
    "USE_WANDB = True\n",
    "\n",
    "DATASET_NAME = \"gretelai/synthetic_text_to_sql\"\n",
    "NUM_EXAMPLES = 300\n",
    "DATASET_SPLIT = \"train\"\n",
    "\n",
    "REWARD_WEIGHTS = {\n",
    "    \"format\": 1.0,\n",
    "    \"sql_correctness\": 1.2,\n",
    "    \"complexity\": 0.6,\n",
    "    \"reasoning\": 0.7,\n",
    "}\n",
    "SYNTAX_PENALTY = -0.1 * REWARD_WEIGHTS[\"sql_correctness\"]\n",
    "\n",
    "if USE_WANDB:\n",
    "    try:\n",
    "        import wandb\n",
    "    except ImportError:\n",
    "        print(\"Wandb not installed. Disabling W&B logging.\")\n",
    "        USE_WANDB = False\n",
    "\n",
    "print(\"\\n=== Starting SQL-to-Text Training Script ===\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Max Sequence Length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"LoRA Rank: {LORA_RANK}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Gradient Accumulation: {GRAD_ACCUMULATION}\")\n",
    "print(f\"Number of Generations: {NUM_GENERATIONS}\")\n",
    "print(f\"Max Steps: {MAX_STEPS}\")\n",
    "print_gpu_memory(\"start\")\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an AI assistant that converts natural language questions into SQL queries compatible with PostgreSQL syntax.\n",
    "Given a database schema and a question, generate the correct PostgreSQL query.\n",
    "\n",
    "Think about the problem and provide your working out.\n",
    "Place it between <reasoning> and </reasoning>.\n",
    "Then, provide your solution between <sql> and </sql>.\n",
    "\n",
    "Here's an example of how you should respond:\n",
    "\n",
    "<reasoning>\n",
    "This database has a users table with columns for id, name, and age.\n",
    "The question asks for all users over 30, so I need to query the users table with a WHERE condition.\n",
    "</reasoning>\n",
    "<sql>\n",
    "SELECT * FROM users WHERE age > 30;\n",
    "</sql>\n",
    "\n",
    "Respond ONLY in the format above, including the <reasoning> and <sql> tags.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_sql(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    match = re.search(r\"<sql>(.*?)</sql>\", text, re.IGNORECASE | re.DOTALL)\n",
    "    if match:\n",
    "        sql = match.group(1).strip()\n",
    "        sql = re.sub(r\"^\\s*--.*?\\n\", \"\", sql)\n",
    "        sql = re.sub(r\"\\n--.*?\\s*$\", \"\", sql)\n",
    "        return sql.strip()\n",
    "    else:\n",
    "        sql_keywords = [\"SELECT \", \"INSERT \", \"UPDATE \", \"DELETE \",\n",
    "                        \"CREATE \", \"ALTER \", \"DROP \", \"TRUNCATE \",\n",
    "                        \"GRANT \", \"REVOKE \", \"MERGE \", \"EXEC \", \"WITH \"]\n",
    "\n",
    "        text_upper = text.upper()\n",
    "        sql_start_index = -1\n",
    "        keyword_found = \"\"\n",
    "\n",
    "        for keyword in sql_keywords:\n",
    "            idx = text_upper.find(keyword)\n",
    "            if idx != -1:\n",
    "                if sql_start_index == -1 or idx < sql_start_index:\n",
    "                    sql_start_index = idx\n",
    "                    keyword_found = keyword\n",
    "\n",
    "        if sql_start_index != -1:\n",
    "            potential_sql = text[sql_start_index:]\n",
    "            if \"</reasoning>\" in potential_sql:\n",
    "                potential_sql = potential_sql.split(\"</reasoning>\", 1)[0]\n",
    "\n",
    "            if \";\" in potential_sql:\n",
    "                potential_sql = potential_sql.split(\";\", 1)[0] + \";\"\n",
    "            return potential_sql.strip()\n",
    "\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_schema_from_context(sql_context: str) -> str:\n",
    "    if not sql_context:\n",
    "        return \"No schema information available.\"\n",
    "    statements = sqlparse.split(sql_context)\n",
    "    schema_statements = [\n",
    "        s.strip() for s in statements\n",
    "        if s.strip().upper().startswith(\"CREATE TABLE\")\n",
    "    ]\n",
    "    extracted_schema = \"\\n\".join(schema_statements)\n",
    "    return extracted_schema if extracted_schema else sql_context\n",
    "\n",
    "\n",
    "def filter_sql_context_for_training(sql_context: str) -> str:\n",
    "    return extract_schema_from_context(sql_context)\n",
    "\n",
    "\n",
    "try:\n",
    "    logger.info(\"=== Loading Model ===\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=MODEL_NAME,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        load_in_4bit=True,\n",
    "        fast_inference=True,\n",
    "        max_lora_rank=LORA_RANK,\n",
    "        dtype=None,\n",
    "    )\n",
    "    logger.info(\"Model loaded successfully\")\n",
    "    print_gpu_memory(\"after model load\")\n",
    "\n",
    "    logger.info(\"=== Applying LoRA ===\")\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=LORA_RANK,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        lora_alpha=LORA_RANK,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=3407,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "    )\n",
    "    logger.info(\"LoRA adapters applied successfully\")\n",
    "    print_gpu_memory(\"after LoRA\")\n",
    "except Exception as e:\n",
    "    logger.error(\n",
    "        f\"Error in model loading or LoRA application: {e}\", exc_info=True)\n",
    "    exit(1)\n",
    "\n",
    "try:\n",
    "    logger.info(\"=== Loading Dataset ===\")\n",
    "    train_df = pd.read_json(TRAIN_DATA_FILE, lines=True)\n",
    "\n",
    "    if NUM_EXAMPLES and len(train_df) > NUM_EXAMPLES:\n",
    "        dataset = train_df.sample(\n",
    "            n=NUM_EXAMPLES, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    dataset = train_df.to_dict(orient='records')\n",
    "\n",
    "    logger.info(f\"Loaded {len(dataset)} examples from {DATASET_NAME}\")\n",
    "    print_gpu_memory(\"after dataset load\")\n",
    "\n",
    "    train_data = []\n",
    "    logger.info(\"=== Preparing Dataset ===\")\n",
    "\n",
    "    for i, example in enumerate(tqdm(dataset, desc=\"Processing examples\")):\n",
    "        sql_prompt = example.get(\"sql_prompt\", \"\")\n",
    "        sql_context = example.get(\"sql_context\", \"\")\n",
    "        gold_sql = example.get(\"sql\", \"\")\n",
    "\n",
    "        if not sql_prompt or not sql_context or not gold_sql:\n",
    "            logger.warning(\n",
    "                f\"Skipping example {i} due to missing data (prompt, context, or gold SQL).\")\n",
    "            continue\n",
    "\n",
    "        filtered_context = filter_sql_context_for_training(sql_context)\n",
    "        schema_for_prompt = extract_schema_from_context(filtered_context)\n",
    "\n",
    "        prompt_chat = [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': f\"{schema_for_prompt}\\n\\nQuestion: {sql_prompt}\"}\n",
    "        ]\n",
    "        prompt_string = tokenizer.apply_chat_template(\n",
    "            prompt_chat, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        train_data.append({\n",
    "            'prompt': prompt_string,\n",
    "            'references': [{\n",
    "                'gold_sql': gold_sql,\n",
    "                'sql_context': sql_context,\n",
    "                'sql_prompt': sql_prompt\n",
    "            }],\n",
    "        })\n",
    "\n",
    "    logger.info(f\"Prepared {len(train_data)} training examples\")\n",
    "    print_gpu_memory(\"after data preparation\")\n",
    "\n",
    "    if not train_data:\n",
    "        logger.error(\n",
    "            \"No valid training data could be prepared. Check dataset format and content.\")\n",
    "        exit(1)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in data preparation: {e}\", exc_info=True)\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "class RewardLoggerCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.step = 0\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        self.step += 1\n",
    "        if self.step % 25 == 0:\n",
    "            logger.info(f\"\\n--- Step {self.step} Reward Details (Sample) ---\")\n",
    "            if 'loss' in state.log_history[-1]:\n",
    "                logger.info(\n",
    "                    f\" Step {self.step}: Current Loss: {state.log_history[-1]['loss']:.4f}\")\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    if USE_WANDB:\n",
    "        try:\n",
    "            if wandb.run is None:\n",
    "                wandb.init(\n",
    "                    project=\"text-to-sql-finetuning\",\n",
    "                    name=f\"sql-grpo-{MODEL_NAME.split('/')[-1]}-{MAX_STEPS}steps\",\n",
    "                    config={\n",
    "                        \"model_name\": MODEL_NAME,\n",
    "                        \"lora_rank\": LORA_RANK,\n",
    "                        \"max_seq_length\": MAX_SEQ_LENGTH,\n",
    "                        \"batch_size\": BATCH_SIZE,\n",
    "                        \"grad_accumulation\": GRAD_ACCUMULATION,\n",
    "                        \"num_generations\": NUM_GENERATIONS,\n",
    "                        \"max_steps\": MAX_STEPS,\n",
    "                        \"dataset\": DATASET_NAME,\n",
    "                        \"num_examples\": NUM_EXAMPLES,\n",
    "                        \"learning_rate\": 5e-6,\n",
    "                        \"weight_decay\": 0.01,\n",
    "                        \"warmup_ratio\": 0.1,\n",
    "                        \"lr_scheduler_type\": \"cosine\",\n",
    "                        \"optim\": \"adamw_8bit\",\n",
    "                        \"syntax_penalty\": SYNTAX_PENALTY,\n",
    "                        \"reward_weights\": REWARD_WEIGHTS,\n",
    "                        \"stage\": \"grpo\",\n",
    "                    },\n",
    "                    resume=\"allow\",\n",
    "                    save_code=True,\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\"WandB already initialized, resuming run.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"WandB initialization failed: {e}\", exc_info=True)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    print_gpu_memory(\"before trainer init\")\n",
    "\n",
    "    effective_max_completion_length = 300\n",
    "    effective_max_prompt_length = MAX_SEQ_LENGTH - \\\n",
    "        effective_max_completion_length - 32\n",
    "\n",
    "    training_args = GRPOConfig(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        learning_rate=5e-6,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "        optim=\"adamw_8bit\",\n",
    "        max_steps=MAX_STEPS,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_steps=5,\n",
    "        save_steps=50,\n",
    "        save_total_limit=2,\n",
    "        save_strategy=\"steps\",\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        max_prompt_length=effective_max_prompt_length,\n",
    "        max_completion_length=effective_max_completion_length,\n",
    "        num_generations=NUM_GENERATIONS,\n",
    "        beta=0.1,\n",
    "        use_vllm=True,\n",
    "        report_to=\"wandb\" if USE_WANDB else \"none\",\n",
    "        remove_unused_columns=False,\n",
    "        seed=42,\n",
    "        dataloader_num_workers=2,\n",
    "        max_grad_norm=1.0,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Initializing GRPOTrainer with improved reward functions...\")\n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        beta=training_args.beta,\n",
    "        processing_class=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        reward_funcs=[\n",
    "            soft_format_reward_func,\n",
    "            execute_query_reward_func,\n",
    "            complexity_reward,\n",
    "            reasoning_quality_reward,\n",
    "        ],\n",
    "        callbacks=[RewardLoggerCallback()] if not USE_WANDB else None,\n",
    "    )\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    print_gpu_memory(\"before training starts\")\n",
    "\n",
    "    logger.info(\"Starting GRPO training...\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "    final_save_path = f\"{OUTPUT_DIR}/final_lora\"\n",
    "    logger.info(f\"Saving final LoRA adapters to {final_save_path}...\")\n",
    "    trainer.save_model(final_save_path)\n",
    "    tokenizer.save_pretrained(final_save_path)\n",
    "    logger.info(\"Model and tokenizer saved.\")\n",
    "\n",
    "    if USE_WANDB and wandb.run:\n",
    "        try:\n",
    "            logger.info(\"Logging final model artifacts to WandB...\")\n",
    "            wandb.save(f\"{final_save_path}/*\")\n",
    "            wandb.finish()\n",
    "            logger.info(\"WandB run finished.\")\n",
    "        except Exception as e:\n",
    "            logger.error(\n",
    "                f\"Failed to finish WandB run or save artifacts: {e}\", exc_info=True)\n",
    "\n",
    "    print_gpu_memory(\"after training\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def test_model(model, tokenizer):\n",
    "    logger.info(\"\\n=== Testing trained model with a sample query ===\")\n",
    "\n",
    "    EVAL_DATA_FILE = \"cleaned_eval_queries.jsonl\"\n",
    "\n",
    "    try:\n",
    "        eval_df = pd.read_json(EVAL_DATA_FILE, lines=True)\n",
    "        if eval_df.empty:\n",
    "            raise ValueError(\n",
    "                f\"Evaluation dataset '{EVAL_DATA_FILE}' is empty.\")\n",
    "\n",
    "        eval_sample = eval_df.sample(n=1, random_state=123).iloc[0]\n",
    "\n",
    "        sql_prompt = eval_sample.get(\"sql_prompt\", \"N/A\")\n",
    "        sql_context = eval_sample.get(\"sql_context\", \"\")\n",
    "        gold_sql = eval_sample.get(\"sql\", \"N/A\")\n",
    "\n",
    "    except (ValueError, FileNotFoundError) as e:\n",
    "        logger.warning(\n",
    "            f\"Could not load eval sample: {e}. Using a default sample.\")\n",
    "        sql_prompt = \"List the names of departments with more than 10 employees.\"\n",
    "        sql_context = \"\"\"\n",
    "        CREATE TABLE departments (department_id INT PRIMARY KEY, name TEXT);\n",
    "        CREATE TABLE employees (employee_id INT PRIMARY KEY, name TEXT, department_id INT, FOREIGN KEY (department_id) REFERENCES departments(department_id));\n",
    "        \"\"\"\n",
    "        gold_sql = \"\"\"\n",
    "        SELECT T1.name FROM departments AS T1 JOIN employees AS T2 ON T1.department_id = T2.department_id GROUP BY T1.department_id HAVING count(*) > 10\n",
    "        \"\"\"\n",
    "\n",
    "    schema_for_prompt = extract_schema_from_context(sql_context)\n",
    "    test_prompt_chat = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"{schema_for_prompt}\\n\\nQuestion: {sql_prompt}\"}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        test_prompt_chat, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    print_gpu_memory(\"before test generation\")\n",
    "\n",
    "    logger.info(\"Generating test response...\")\n",
    "    output_text = \"[Generation Failed]\"\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                               max_length=MAX_SEQ_LENGTH).to(model.device)\n",
    "\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=300,\n",
    "                temperature=0.2,\n",
    "                top_p=0.95,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            input_length = inputs['input_ids'].shape[1]\n",
    "            generated_tokens = output_ids[0][input_length:]\n",
    "            output_text = tokenizer.decode(\n",
    "                generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during test generation: {e}\", exc_info=True)\n",
    "\n",
    "    print(\"\\n--- Test Results ---\")\n",
    "    print(f\"Question: {sql_prompt}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Gold SQL:\\n{gold_sql}\")\n",
    "    print(\"-\" * 40)\n",
    "    generated_sql = extract_sql(output_text)\n",
    "    print(\n",
    "        f\"Generated SQL:\\n{generated_sql if generated_sql else '[No SQL Extracted]'}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Full Generated Output:\\n{output_text}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print_gpu_memory(\"after test\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model, trained_tokenizer = train_model()\n",
    "\n",
    "    if trained_model and trained_tokenizer:\n",
    "        test_model(trained_model, trained_tokenizer)\n",
    "    else:\n",
    "        logger.error(\n",
    "            \"Training did not return a valid model or tokenizer. Skipping test.\")\n",
    "\n",
    "    logger.info(\"\\nGRPO Training Script Completed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
